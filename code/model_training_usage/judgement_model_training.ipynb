{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiyer training on judgement data\n",
    "\n",
    "Based on the work of Mochales Palau and Moens [1] two classifyer are trained on the judgment corpus.  \n",
    "The first classifyer decides weather the text is a subsumtion/definition or something else. (Correlating to the detection of argumentative text in the base paper.)  \n",
    "The second classifyer detects if a sentence belongs to a subsupmtion or a definition. The decision of the first classifyer is fed into the second classification.  \n",
    "&nbsp;  \n",
    "&nbsp;  \n",
    "<sub>[1] Palau, Raquel Mochales, and Marie-Francine Moens. \"Argumentation mining: the detection, classification and structure of arguments in text.\" Proceedings of the 12th international conference on artificial intelligence and law. 2009.</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "# all imports for the notebook\n",
    "\n",
    "# data handling\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# natural language processing\n",
    "from somajo import SoMaJo # https://github.com/tsproisl/SoMaJo\n",
    "import treetaggerwrapper\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# system paths\n",
    "import os\n",
    "\n",
    "# miscellaneous\n",
    "import pprint\n",
    "import itertools\n",
    "import re\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation 1\n",
    "\n",
    "In this step the judgments JSON files are loaded and all not neccessary data are dropped.  \n",
    "Furthermore, the text is split into senteces and each sentence is assigned a label.  \n",
    "\n",
    "Afterwards, the corpus is flattened for the first round of feature engineering. Since the first classifcation is a binary one the features are turned into binary features: interestin/non interesting for later classification.  \n",
    "In the end the corpus is split into train and test data with a 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'thesis_corpus/'\n",
    "\n",
    "# flatten corpus for generating basic features and extract flattende labels\n",
    "corpus = []\n",
    "corpus_labels = []\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    with open(path+file, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        reasons = data['decision_text']['decision_reasons']\n",
    "    \n",
    "    for reason in reasons:\n",
    "        for sentence in reason:\n",
    "            corpus.append(sentence[0])\n",
    "            if sentence[1] in ['definition', 'subsumption']:\n",
    "                corpus_labels.append('interest')\n",
    "            else:\n",
    "                corpus_labels.append('no_interest')\n",
    "        \n",
    "X = np.array(corpus)\n",
    "y = np.array(corpus_labels)\n",
    "                \n",
    "# train/test split with 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering 1\n",
    "\n",
    "The first classifyer needs the following features:\n",
    "- Unigrams\n",
    "- Bigrams\n",
    "- Trigrams\n",
    "- Adverbs\n",
    "- Verbs\n",
    "- Modal Auxiliary\n",
    "- Word Couples\n",
    "- Text Statistics\n",
    "- Punctuation\n",
    "\n",
    "In this step classes are defined that extract the neccessary features.  \n",
    "For unigrams, bigrams and trigrams the CountVectorizer implementation from scikit learn is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Adverbs, Verbs and Modal Auxiliary\n",
    "\n",
    "extracting adverbs, verbs, modal auxiliary by using the TreeTagger (https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POS_extractor:  \n",
    "    \n",
    "    # ADV for adverbs, VV for verbs, VM for modal auxuliary\n",
    "    # learn a vocabluary of all choosen POS\n",
    "    def fit(corpus, POS_abbreviation):\n",
    "        tagger = treetaggerwrapper.TreeTagger(TAGLANG='de')\n",
    "        POS = []\n",
    "        for item in corpus:\n",
    "            tags = tagger.tag_text(item)\n",
    "            tags2 = treetaggerwrapper.make_tags(tags)\n",
    "\n",
    "            for word in tags2:\n",
    "                if type(word) == treetaggerwrapper.Tag:\n",
    "                    if POS_abbreviation in word[1]:\n",
    "                        POS.append(word[0])\n",
    "        return list(set(POS))\n",
    "\n",
    "    # transforms sentences to sentence term matrix\n",
    "    def transform(sentences, vocabluary, POS_abbreviation):\n",
    "        POS_vec = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sent_vec = []\n",
    "            \n",
    "            # compose binary modal auxiliary vector for each sentence\n",
    "            if POS_abbreviation == 'VM':\n",
    "                if any(mod in sentence for mod in vocabluary):\n",
    "                    POS_vec.append([1])\n",
    "                else:\n",
    "                    POS_vec.append([-1])\n",
    "            \n",
    "            # compose adverbs/verbs vector for each sentence\n",
    "            else:\n",
    "                for i in range(len(vocabluary)):\n",
    "                    if vocabluary[i] in sentence:\n",
    "                        sent_vec.append(1)\n",
    "                    else:\n",
    "                        sent_vec.append(0)\n",
    "                POS_vec.append(sent_vec)\n",
    "        return POS_vec\n",
    "            \n",
    "# sources:\n",
    "# https://treetaggerwrapper.readthedocs.io/en/latest/\n",
    "# https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/\n",
    "# https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/stts_guide.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Couple Feature\n",
    "\n",
    "This feature has to be excluded due to computing reasons. With a simple 80/20 train/test split a vocabluary of the size 2,105,114 was created. With 5,306 sentences in the train corpus a computation of the sentence-couple matrix is not possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_Couple:\n",
    "    \n",
    "    # learn a vocabluary of all word couples\n",
    "    def fit(sentences):\n",
    "        word_couples = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # buid permutations of 2 over the sentence that is converted to a list\n",
    "            for word in itertools.permutations(sentence.split(' '), 2):\n",
    "                word_couples.append(word)\n",
    "\n",
    "        word_couples = list(set(word_couples))\n",
    "        word_couples = sorted(word_couples, key=lambda tup: (tup[0],tup[1]))\n",
    "        \n",
    "        return word_couples\n",
    "    \n",
    "    # transform the input text into a couple sentence matrix    \n",
    "    def transform(sentences, vocabulary):\n",
    "        word_couples_vec = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            word_couples_sent_vec = []\n",
    "            couples_sent = list(itertools.permutations(sentence.split(' '), 2))\n",
    "\n",
    "            # compose word couple vector for each sentence\n",
    "            for couple in vocabulary:\n",
    "                if couple in couples_sent:\n",
    "                    word_couples_sent_vec.append(1)\n",
    "                else:\n",
    "                    word_couples_sent_vec.append(0)\n",
    "            word_couples_vec.append(word_couples_sent_vec)\n",
    "            \n",
    "        return word_couples_vec  \n",
    "        \n",
    "# sources:\n",
    "# https://stackoverflow.com/questions/942543/operation-on-every-pair-of-element-in-a-list\n",
    "# https://stackoverflow.com/questions/3121979/how-to-sort-a-list-tuple-of-lists-tuples-by-the-element-at-a-given-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "coup = Word_Couple\n",
    "vocab = coup.fit(X_train)\n",
    "train = coup.transform(X_train[0], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2118453\n"
     ]
    }
   ],
   "source": [
    "print(len(train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exracting Sentence Statistics\n",
    "\n",
    "In this function sentence length, average word length and number of punctuation marks are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence_Statistics:\n",
    "    \n",
    "    # extract all neccessary information from the corpus\n",
    "    def transform(sentences):\n",
    "        sentence_statistics = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sent_length = len(sentence.split(' '))\n",
    "            num_punctuation = len((re.findall(r'[^äöüÄÖÜa-zA-Z0-9%€&$#\\+ ]', sentence)))\n",
    "            len_words = 0\n",
    "\n",
    "            for item in sentence.split(' '):\n",
    "                len_words += len(item)\n",
    "            avg_word_length = len_words/len(sentence.split(' '))\n",
    "\n",
    "            sentence_statistics.append([sent_length, avg_word_length, num_punctuation])\n",
    "        return sentence_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Punctuation Sequence\n",
    "\n",
    "In this function the punctuation sequence is extracted an encoded.  \n",
    "The following encoding is used\n",
    "- \\+ --> 0\n",
    "- . --> 1 \n",
    "- ! --> 2\n",
    "- ? --> 3\n",
    "- , --> 4\n",
    "- ; --> 5\n",
    "- : --> 6\n",
    "- / --> 7\n",
    "- § --> 8\n",
    "\n",
    "Only the most common punctuation is considered due to encoding reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Punctuation_Sequence:\n",
    "    \n",
    "    def transform(sentences):\n",
    "\n",
    "        punctuation_vec = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence = re.sub(r'([^äöüÄÖÜa-zA-Z0-9%€&$#\\+ ])[^äöüÄÖÜa-zA-Z0-9%€&$#\\+ ]+', r'\\1+', sentence)\n",
    "            sentence = re.findall(r'[\\.\\!\\?,;:/§\\+]', sentence)\n",
    "            sentence = ''.join(sentence)\n",
    "\n",
    "            sentence = re.sub(r'\\+', '0', sentence)\n",
    "            sentence = re.sub(r'\\.', '1', sentence)\n",
    "            sentence = re.sub(r'\\!', '2', sentence)\n",
    "            sentence = re.sub(r'\\?', '3', sentence)\n",
    "            sentence = re.sub(r',', '4', sentence)\n",
    "            sentence = re.sub(r';', '5', sentence)\n",
    "            sentence = re.sub(r':', '6', sentence)\n",
    "            sentence = re.sub(r'/', '7', sentence)\n",
    "            sentence = re.sub(r'§', '8', sentence)\n",
    "            \n",
    "            if sentence == '':\n",
    "                punctuation_vec.append([-1])\n",
    "            else:\n",
    "                punctuation_vec.append([int(sentence)])\n",
    "            \n",
    "        return punctuation_vec\n",
    "    \n",
    "\n",
    "# source: https://note.nkmk.me/en/python-str-replace-translate-re-sub/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation 1\n",
    "\n",
    "The first model is a maximum entropy model, another name for Logistic Regression.  \n",
    "The data is evaluated on the initial 80/20 train test split.  \n",
    "Furthermore, five fold crossvalidation is done on the test set to get clearere insights into the data. In the scikit learn implementattion of KFold 5 is the standard parameter for the folds, therefore, five fold are choosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.90      0.93      0.91      3001\n",
      " no_interest       0.78      0.69      0.73      1031\n",
      "\n",
      "    accuracy                           0.87      4032\n",
      "   macro avg       0.84      0.81      0.82      4032\n",
      "weighted avg       0.87      0.87      0.87      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.90      0.93      0.91      2961\n",
      " no_interest       0.78      0.71      0.74      1071\n",
      "\n",
      "    accuracy                           0.87      4032\n",
      "   macro avg       0.84      0.82      0.83      4032\n",
      "weighted avg       0.87      0.87      0.87      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.90      0.93      0.91      2996\n",
      " no_interest       0.77      0.71      0.74      1036\n",
      "\n",
      "    accuracy                           0.87      4032\n",
      "   macro avg       0.84      0.82      0.83      4032\n",
      "weighted avg       0.87      0.87      0.87      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.89      0.92      0.91      2966\n",
      " no_interest       0.76      0.70      0.73      1065\n",
      "\n",
      "    accuracy                           0.86      4031\n",
      "   macro avg       0.83      0.81      0.82      4031\n",
      "weighted avg       0.86      0.86      0.86      4031\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.91      0.93      0.92      3012\n",
      " no_interest       0.78      0.71      0.74      1019\n",
      "\n",
      "    accuracy                           0.88      4031\n",
      "   macro avg       0.84      0.82      0.83      4031\n",
      "weighted avg       0.87      0.88      0.87      4031\n",
      "\n",
      "Precision: 0.8367147081975231\n",
      "Recall: 0.8162073514413661\n",
      "F1-Score: 0.8255253428288473\n",
      "Accuracy: 0.8700267495363315\n"
     ]
    }
   ],
   "source": [
    "kf = KFold()\n",
    "\n",
    "acc = 0\n",
    "pre = 0\n",
    "rec = 0\n",
    "f_1 = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_training, X_testing = X_train[train_index], X_train[test_index]\n",
    "    y_training, y_testing = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    vectorizer_uni = CountVectorizer(analyzer='word', lowercase=False, ngram_range=(1, 1))\n",
    "    unigrams_train = vectorizer_uni.fit_transform(X_training)\n",
    "    unigrams_test = vectorizer_uni.transform(X_testing)\n",
    "\n",
    "    clf_uni = LogisticRegression( max_iter=1000).fit(unigrams_train, y_training)\n",
    "    uni_predict = clf_uni.predict(unigrams_test)\n",
    "\n",
    "    acc += metrics.accuracy_score(y_testing, uni_predict)   \n",
    "    metric = metrics.precision_recall_fscore_support(y_testing, uni_predict, average='macro')\n",
    "    pre += metric[0]\n",
    "    rec += metric[1]\n",
    "    f_1 += metric[2]\n",
    "    print(metrics.classification_report(y_testing, uni_predict))\n",
    "    \n",
    "print('Precision: ' + str(pre/5))\n",
    "print('Recall: ' + str(rec/5))\n",
    "print('F1-Score: ' + str(f_1/5))\n",
    "print('Accuracy: ' + str(acc/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.89      0.91      0.90      3001\n",
      " no_interest       0.71      0.66      0.69      1031\n",
      "\n",
      "    accuracy                           0.84      4032\n",
      "   macro avg       0.80      0.79      0.79      4032\n",
      "weighted avg       0.84      0.84      0.84      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.89      0.89      0.89      2961\n",
      " no_interest       0.70      0.68      0.69      1071\n",
      "\n",
      "    accuracy                           0.84      4032\n",
      "   macro avg       0.79      0.79      0.79      4032\n",
      "weighted avg       0.84      0.84      0.84      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.89      0.90      0.89      2996\n",
      " no_interest       0.69      0.68      0.68      1036\n",
      "\n",
      "    accuracy                           0.84      4032\n",
      "   macro avg       0.79      0.79      0.79      4032\n",
      "weighted avg       0.84      0.84      0.84      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.89      0.90      0.89      2966\n",
      " no_interest       0.70      0.68      0.69      1065\n",
      "\n",
      "    accuracy                           0.84      4031\n",
      "   macro avg       0.79      0.79      0.79      4031\n",
      "weighted avg       0.84      0.84      0.84      4031\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.89      0.90      0.90      3012\n",
      " no_interest       0.69      0.68      0.69      1019\n",
      "\n",
      "    accuracy                           0.84      4031\n",
      "   macro avg       0.79      0.79      0.79      4031\n",
      "weighted avg       0.84      0.84      0.84      4031\n",
      "\n",
      "Precision: 0.7934609670402933\n",
      "Recall: 0.7870347811609465\n",
      "F1-Score: 0.7901118273107615\n",
      "Accuracy: 0.8406091752213992\n"
     ]
    }
   ],
   "source": [
    "# bigrams\n",
    "\n",
    "kf = KFold()\n",
    "\n",
    "acc = 0\n",
    "pre = 0\n",
    "rec = 0\n",
    "f_1 = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_training, X_testing = X_train[train_index], X_train[test_index]\n",
    "    y_training, y_testing = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    vectorizer_bi = CountVectorizer(analyzer='word', lowercase=False, ngram_range=(2, 2))\n",
    "    bigrams_train = vectorizer_bi.fit_transform(X_training)\n",
    "    bigrams_test = vectorizer_bi.transform(X_testing)\n",
    "\n",
    "    clf_bi = LogisticRegression().fit(bigrams_train, y_training)\n",
    "    bi_predict = clf_bi.predict(bigrams_test)\n",
    "\n",
    "    acc += metrics.accuracy_score(y_testing, bi_predict)   \n",
    "    metric = metrics.precision_recall_fscore_support(y_testing, bi_predict, average='macro')\n",
    "    pre += metric[0]\n",
    "    rec += metric[1]\n",
    "    f_1 += metric[2]\n",
    "    print(metrics.classification_report(y_testing,  bi_predict))\n",
    "    \n",
    "print('Precision: ' + str(pre/5))\n",
    "print('Recall: ' + str(rec/5))\n",
    "print('F1-Score: ' + str(f_1/5))\n",
    "print('Accuracy: ' + str(acc/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.81      0.96      0.88      3001\n",
      " no_interest       0.76      0.34      0.47      1031\n",
      "\n",
      "    accuracy                           0.80      4032\n",
      "   macro avg       0.78      0.65      0.68      4032\n",
      "weighted avg       0.80      0.80      0.78      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.80      0.96      0.87      2961\n",
      " no_interest       0.77      0.34      0.47      1071\n",
      "\n",
      "    accuracy                           0.80      4032\n",
      "   macro avg       0.79      0.65      0.67      4032\n",
      "weighted avg       0.79      0.80      0.77      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.81      0.96      0.88      2996\n",
      " no_interest       0.74      0.33      0.46      1036\n",
      "\n",
      "    accuracy                           0.80      4032\n",
      "   macro avg       0.78      0.65      0.67      4032\n",
      "weighted avg       0.79      0.80      0.77      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.80      0.96      0.87      2966\n",
      " no_interest       0.76      0.34      0.47      1065\n",
      "\n",
      "    accuracy                           0.80      4031\n",
      "   macro avg       0.78      0.65      0.67      4031\n",
      "weighted avg       0.79      0.80      0.77      4031\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.81      0.96      0.88      3012\n",
      " no_interest       0.75      0.35      0.48      1019\n",
      "\n",
      "    accuracy                           0.81      4031\n",
      "   macro avg       0.78      0.65      0.68      4031\n",
      "weighted avg       0.80      0.81      0.78      4031\n",
      "\n",
      "Precision: 0.7819168561305203\n",
      "Recall: 0.6504656957898648\n",
      "F1-Score: 0.6727515193876812\n",
      "Accuracy: 0.8006251279764365\n"
     ]
    }
   ],
   "source": [
    "# trigrams\n",
    "\n",
    "kf = KFold()\n",
    "\n",
    "acc = 0\n",
    "pre = 0\n",
    "rec = 0\n",
    "f_1 = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_training, X_testing = X_train[train_index], X_train[test_index]\n",
    "    y_training, y_testing = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    vectorizer_tri = CountVectorizer(analyzer='word', lowercase=False, ngram_range=(3, 3))\n",
    "    trigrams_train = vectorizer_tri.fit_transform(X_training)\n",
    "    trigrams_test = vectorizer_tri.transform(X_testing)\n",
    "\n",
    "    clf_tri = LogisticRegression().fit(trigrams_train, y_training)\n",
    "    tri_predict = clf_tri.predict(trigrams_test)\n",
    "\n",
    "    acc += metrics.accuracy_score(y_testing, tri_predict)   \n",
    "    metric = metrics.precision_recall_fscore_support(y_testing, tri_predict, average='macro')\n",
    "    pre += metric[0]\n",
    "    rec += metric[1]\n",
    "    f_1 += metric[2]\n",
    "    print(metrics.classification_report(y_testing, tri_predict))\n",
    "    \n",
    "print('Precision: ' + str(pre/5))\n",
    "print('Recall: ' + str(rec/5))\n",
    "print('F1-Score: ' + str(f_1/5))\n",
    "print('Accuracy: ' + str(acc/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.83      0.91      0.87      3001\n",
      " no_interest       0.65      0.47      0.54      1031\n",
      "\n",
      "    accuracy                           0.80      4032\n",
      "   macro avg       0.74      0.69      0.71      4032\n",
      "weighted avg       0.79      0.80      0.79      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.82      0.92      0.87      2961\n",
      " no_interest       0.68      0.45      0.54      1071\n",
      "\n",
      "    accuracy                           0.80      4032\n",
      "   macro avg       0.75      0.69      0.71      4032\n",
      "weighted avg       0.79      0.80      0.78      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.82      0.92      0.87      2996\n",
      " no_interest       0.66      0.43      0.52      1036\n",
      "\n",
      "    accuracy                           0.80      4032\n",
      "   macro avg       0.74      0.68      0.70      4032\n",
      "weighted avg       0.78      0.80      0.78      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.82      0.93      0.87      2966\n",
      " no_interest       0.67      0.42      0.51      1065\n",
      "\n",
      "    accuracy                           0.79      4031\n",
      "   macro avg       0.74      0.67      0.69      4031\n",
      "weighted avg       0.78      0.79      0.77      4031\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.83      0.92      0.87      3012\n",
      " no_interest       0.65      0.44      0.52      1019\n",
      "\n",
      "    accuracy                           0.80      4031\n",
      "   macro avg       0.74      0.68      0.70      4031\n",
      "weighted avg       0.78      0.80      0.78      4031\n",
      "\n",
      "Precision: 0.7427741699890651\n",
      "Recall: 0.6807841492935405\n",
      "F1-Score: 0.699296229552913\n",
      "Accuracy: 0.7964082305584104\n"
     ]
    }
   ],
   "source": [
    "# adverbs\n",
    "\n",
    "kf = KFold()\n",
    "\n",
    "acc = 0\n",
    "pre = 0\n",
    "rec = 0\n",
    "f_1 = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_training, X_testing = X_train[train_index], X_train[test_index]\n",
    "    y_training, y_testing = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    adverb_extractor = POS_extractor\n",
    "    adverbs_voc = adverb_extractor.fit(X_training, 'ADV')\n",
    "    adverbs_vec_train = adverb_extractor.transform(X_training, adverbs_voc, 'ADV')\n",
    "    adverbs_vec_test = adverb_extractor.transform(X_testing, adverbs_voc, 'ADV')\n",
    "\n",
    "    clf_adv = LogisticRegression().fit(adverbs_vec_train, y_training)\n",
    "    adv_predict = clf_adv.predict(adverbs_vec_test)\n",
    "\n",
    "    acc += metrics.accuracy_score(y_testing, adv_predict)   \n",
    "    metric = metrics.precision_recall_fscore_support(y_testing, adv_predict, average='macro')\n",
    "    pre += metric[0]\n",
    "    rec += metric[1]\n",
    "    f_1 += metric[2]\n",
    "    print(metrics.classification_report(y_testing, adv_predict))\n",
    "    \n",
    "\n",
    "print('Precision: ' + str(pre/5))\n",
    "print('Recall: ' + str(rec/5))\n",
    "print('F1-Score: ' + str(f_1/5))\n",
    "print('Accuracy: ' + str(acc/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.85      0.94      0.89      3001\n",
      " no_interest       0.75      0.52      0.61      1031\n",
      "\n",
      "    accuracy                           0.83      4032\n",
      "   macro avg       0.80      0.73      0.75      4032\n",
      "weighted avg       0.82      0.83      0.82      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.85      0.93      0.89      2961\n",
      " no_interest       0.74      0.54      0.62      1071\n",
      "\n",
      "    accuracy                           0.83      4032\n",
      "   macro avg       0.79      0.74      0.76      4032\n",
      "weighted avg       0.82      0.83      0.82      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.85      0.93      0.89      2996\n",
      " no_interest       0.73      0.52      0.61      1036\n",
      "\n",
      "    accuracy                           0.83      4032\n",
      "   macro avg       0.79      0.73      0.75      4032\n",
      "weighted avg       0.82      0.83      0.82      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.85      0.94      0.89      2966\n",
      " no_interest       0.76      0.55      0.64      1065\n",
      "\n",
      "    accuracy                           0.84      4031\n",
      "   macro avg       0.81      0.74      0.77      4031\n",
      "weighted avg       0.83      0.84      0.83      4031\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.86      0.94      0.90      3012\n",
      " no_interest       0.75      0.55      0.64      1019\n",
      "\n",
      "    accuracy                           0.84      4031\n",
      "   macro avg       0.80      0.74      0.77      4031\n",
      "weighted avg       0.83      0.84      0.83      4031\n",
      "\n",
      "Precision: 0.7991247785782448\n",
      "Recall: 0.7361270344948534\n",
      "F1-Score: 0.7580159346361878\n",
      "Accuracy: 0.83252357473627\n"
     ]
    }
   ],
   "source": [
    "# verbs\n",
    "\n",
    "kf = KFold()\n",
    "\n",
    "acc = 0\n",
    "pre = 0\n",
    "rec = 0\n",
    "f_1 = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_training, X_testing = X_train[train_index], X_train[test_index]\n",
    "    y_training, y_testing = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    verb_extractor = POS_extractor\n",
    "    verbs_voc = verb_extractor.fit(X_training, 'VV')\n",
    "    verbs_vec_train = verb_extractor.transform(X_training, verbs_voc, 'VV')\n",
    "    verbs_vec_test = verb_extractor.transform(X_testing, verbs_voc, 'VV')\n",
    "\n",
    "    clf_ver = LogisticRegression(max_iter=1000).fit(verbs_vec_train, y_training)\n",
    "    ver_predict = clf_ver.predict(verbs_vec_test)\n",
    "\n",
    "    acc += metrics.accuracy_score(y_testing, ver_predict)   \n",
    "    metric = metrics.precision_recall_fscore_support(y_testing, ver_predict, average='macro')\n",
    "    pre += metric[0]\n",
    "    rec += metric[1]\n",
    "    f_1 += metric[2]\n",
    "    print(metrics.classification_report(y_testing, ver_predict))\n",
    "    \n",
    "print('Precision: ' + str(pre/5))\n",
    "print('Recall: ' + str(rec/5))\n",
    "print('F1-Score: ' + str(f_1/5))\n",
    "print('Accuracy: ' + str(acc/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.74      1.00      0.85      3001\n",
      " no_interest       0.00      0.00      0.00      1031\n",
      "\n",
      "    accuracy                           0.74      4032\n",
      "   macro avg       0.37      0.50      0.43      4032\n",
      "weighted avg       0.55      0.74      0.64      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.73      1.00      0.85      2961\n",
      " no_interest       0.00      0.00      0.00      1071\n",
      "\n",
      "    accuracy                           0.73      4032\n",
      "   macro avg       0.37      0.50      0.42      4032\n",
      "weighted avg       0.54      0.73      0.62      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.74      1.00      0.85      2996\n",
      " no_interest       0.00      0.00      0.00      1036\n",
      "\n",
      "    accuracy                           0.74      4032\n",
      "   macro avg       0.37      0.50      0.43      4032\n",
      "weighted avg       0.55      0.74      0.63      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.74      1.00      0.85      2966\n",
      " no_interest       0.00      0.00      0.00      1065\n",
      "\n",
      "    accuracy                           0.74      4031\n",
      "   macro avg       0.37      0.50      0.42      4031\n",
      "weighted avg       0.54      0.74      0.62      4031\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.75      1.00      0.86      3012\n",
      " no_interest       0.00      0.00      0.00      1019\n",
      "\n",
      "    accuracy                           0.75      4031\n",
      "   macro avg       0.37      0.50      0.43      4031\n",
      "weighted avg       0.56      0.75      0.64      4031\n",
      "\n",
      "Precision: 0.37047328885659947\n",
      "Recall: 0.5\n",
      "F1-Score: 0.4255951110124684\n",
      "Accuracy: 0.7409465777131989\n"
     ]
    }
   ],
   "source": [
    "# modal auxiliary\n",
    "\n",
    "kf = KFold()\n",
    "\n",
    "acc = 0\n",
    "pre = 0\n",
    "rec = 0\n",
    "f_1 = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_training, X_testing = X_train[train_index], X_train[test_index]\n",
    "    y_training, y_testing = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    mod_aux_extractor = POS_extractor\n",
    "    mod_aux_voc = mod_aux_extractor.fit(X_training, 'VM')\n",
    "    mod_aux_vec_train = mod_aux_extractor.transform(X_training, mod_aux_voc, 'VM')\n",
    "    mod_aux_vec_test = mod_aux_extractor.transform(X_testing, mod_aux_voc, 'VM')\n",
    "\n",
    "    clf_ma = LogisticRegression(max_iter=1000).fit(mod_aux_vec_train, y_training)\n",
    "    ma_predict = clf_ma.predict(mod_aux_vec_test)\n",
    "\n",
    "    acc += metrics.accuracy_score(y_testing, ma_predict)   \n",
    "    metric = metrics.precision_recall_fscore_support(y_testing, ma_predict, average='macro')\n",
    "    pre += metric[0]\n",
    "    rec += metric[1]\n",
    "    f_1 += metric[2]\n",
    "    print(metrics.classification_report(y_testing, ma_predict))\n",
    "    \n",
    "print('Precision: ' + str(pre/5))\n",
    "print('Recall: ' + str(rec/5))\n",
    "print('F1-Score: ' + str(f_1/5))\n",
    "print('Accuracy: ' + str(acc/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.81      0.97      0.88      3001\n",
      " no_interest       0.79      0.32      0.46      1031\n",
      "\n",
      "    accuracy                           0.80      4032\n",
      "   macro avg       0.80      0.65      0.67      4032\n",
      "weighted avg       0.80      0.80      0.77      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.79      0.97      0.87      2961\n",
      " no_interest       0.78      0.30      0.43      1071\n",
      "\n",
      "    accuracy                           0.79      4032\n",
      "   macro avg       0.79      0.63      0.65      4032\n",
      "weighted avg       0.79      0.79      0.75      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.81      0.97      0.88      2996\n",
      " no_interest       0.80      0.32      0.46      1036\n",
      "\n",
      "    accuracy                           0.80      4032\n",
      "   macro avg       0.80      0.65      0.67      4032\n",
      "weighted avg       0.80      0.80      0.77      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.79      0.96      0.87      2966\n",
      " no_interest       0.72      0.31      0.43      1065\n",
      "\n",
      "    accuracy                           0.78      4031\n",
      "   macro avg       0.76      0.63      0.65      4031\n",
      "weighted avg       0.77      0.78      0.75      4031\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.81      0.97      0.88      3012\n",
      " no_interest       0.79      0.32      0.46      1019\n",
      "\n",
      "    accuracy                           0.81      4031\n",
      "   macro avg       0.80      0.65      0.67      4031\n",
      "weighted avg       0.80      0.81      0.78      4031\n",
      "\n",
      "Precision: 0.7873102623157389\n",
      "Recall: 0.6410084274178507\n",
      "F1-Score: 0.6618124769313218\n",
      "Accuracy: 0.7983428405059204\n"
     ]
    }
   ],
   "source": [
    "# sentence statistics\n",
    "\n",
    "kf = KFold()\n",
    "\n",
    "acc = 0\n",
    "pre = 0\n",
    "rec = 0\n",
    "f_1 = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_training, X_testing = X_train[train_index], X_train[test_index]\n",
    "    y_training, y_testing = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    sent_stat_extractor = Sentence_Statistics\n",
    "    sent_stat_train = sent_stat_extractor.transform(X_training)\n",
    "    sent_stat_test = sent_stat_extractor.transform(X_testing)\n",
    "\n",
    "    clf_sent_stat = LogisticRegression().fit(sent_stat_train, y_training)\n",
    "    sent_stat_predict = clf_sent_stat.predict(sent_stat_test)\n",
    "\n",
    "    acc += metrics.accuracy_score(y_testing, sent_stat_predict)   \n",
    "    metric = metrics.precision_recall_fscore_support(y_testing, sent_stat_predict, average='macro')\n",
    "    pre += metric[0]\n",
    "    rec += metric[1]\n",
    "    f_1 += metric[2]\n",
    "    print(metrics.classification_report(y_testing, sent_stat_predict))\n",
    "    \n",
    "print('Precision: ' + str(pre/5))\n",
    "print('Recall: ' + str(rec/5))\n",
    "print('F1-Score: ' + str(f_1/5))\n",
    "print('Accuracy: ' + str(acc/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.74      1.00      0.85      3001\n",
      " no_interest       0.00      0.00      0.00      1031\n",
      "\n",
      "    accuracy                           0.74      4032\n",
      "   macro avg       0.37      0.50      0.43      4032\n",
      "weighted avg       0.55      0.74      0.64      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.73      1.00      0.85      2961\n",
      " no_interest       0.00      0.00      0.00      1071\n",
      "\n",
      "    accuracy                           0.73      4032\n",
      "   macro avg       0.37      0.50      0.42      4032\n",
      "weighted avg       0.54      0.73      0.62      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.74      1.00      0.85      2996\n",
      " no_interest       0.00      0.00      0.00      1036\n",
      "\n",
      "    accuracy                           0.74      4032\n",
      "   macro avg       0.37      0.50      0.43      4032\n",
      "weighted avg       0.55      0.74      0.63      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.74      1.00      0.85      2966\n",
      " no_interest       0.00      0.00      0.00      1065\n",
      "\n",
      "    accuracy                           0.74      4031\n",
      "   macro avg       0.37      0.50      0.42      4031\n",
      "weighted avg       0.54      0.74      0.62      4031\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.75      1.00      0.86      3012\n",
      " no_interest       0.00      0.00      0.00      1019\n",
      "\n",
      "    accuracy                           0.75      4031\n",
      "   macro avg       0.37      0.50      0.43      4031\n",
      "weighted avg       0.56      0.75      0.64      4031\n",
      "\n",
      "Precision: 0.37047328885659947\n",
      "Recall: 0.5\n",
      "F1-Score: 0.4255951110124684\n",
      "Accuracy: 0.7409465777131989\n"
     ]
    }
   ],
   "source": [
    "# punctuation sequence\n",
    "\n",
    "kf = KFold()\n",
    "\n",
    "acc = 0\n",
    "pre = 0\n",
    "rec = 0\n",
    "f_1 = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_training, X_testing = X_train[train_index], X_train[test_index]\n",
    "    y_training, y_testing = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    pun_extractor = Punctuation_Sequence\n",
    "    pun_train = pun_extractor.transform(X_training)\n",
    "    pun_test = pun_extractor.transform(X_testing)\n",
    "\n",
    "    clf_pun = LogisticRegression().fit(pun_train, y_training)\n",
    "    pun_predict = clf_pun.predict(pun_test)\n",
    "\n",
    "    acc += metrics.accuracy_score(y_testing, pun_predict)   \n",
    "    metric = metrics.precision_recall_fscore_support(y_testing, pun_predict, average='macro')\n",
    "    pre += metric[0]\n",
    "    rec += metric[1]\n",
    "    f_1 += metric[2]\n",
    "    print(metrics.classification_report(y_testing, pun_predict))\n",
    "    \n",
    "\n",
    "print('Precision: ' + str(pre/5))\n",
    "print('Recall: ' + str(rec/5))\n",
    "print('F1-Score: ' + str(f_1/5))\n",
    "print('Accuracy: ' + str(acc/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    interest       0.74      1.00      0.85      3753\n",
      " no_interest       0.00      0.00      0.00      1287\n",
      "\n",
      "    accuracy                           0.74      5040\n",
      "   macro avg       0.37      0.50      0.43      5040\n",
      "weighted avg       0.55      0.74      0.64      5040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combination of all features with 80/20 split\n",
    "\n",
    "vectorizer_uni = CountVectorizer(analyzer='word', lowercase=False, ngram_range=(1, 1))\n",
    "vectorizer_bi = CountVectorizer(analyzer='word', lowercase=False, ngram_range=(2, 2))\n",
    "vectorizer_tri = CountVectorizer(analyzer='word', lowercase=False, ngram_range=(3, 3))\n",
    "adverb_extractor = POS_extractor\n",
    "verb_extractor = POS_extractor\n",
    "mod_aux_extractor = POS_extractor\n",
    "sent_stat_extractor = Sentence_Statistics\n",
    "pun_extractor = Punctuation_Sequence\n",
    "    \n",
    "unigrams_train = vectorizer_uni.fit_transform(X_train).toarray().tolist()\n",
    "unigrams_test = vectorizer_uni.transform(X_test).toarray().tolist()\n",
    "# bigrams_train = vectorizer_bi.fit_transform(X_train).toarray().tolist()\n",
    "# bigrams_test = vectorizer_bi.transform(X_test).toarray().tolist()\n",
    "# trigrams_train = vectorizer_tri.fit_transform(X_training).toarray().tolist()\n",
    "# trigrams_test = vectorizer_tri.transform(X_test).toarray().tolist()\n",
    "adverbs_voc = adverb_extractor.fit(X_train, 'ADV')\n",
    "adverbs_vec_train = adverb_extractor.transform(X_train, adverbs_voc, 'ADV')\n",
    "adverbs_vec_test = adverb_extractor.transform(X_test, adverbs_voc, 'ADV')\n",
    "verbs_voc = verb_extractor.fit(X_train, 'VV')\n",
    "verbs_vec_train = verb_extractor.transform(X_train, verbs_voc, 'VV')\n",
    "verbs_vec_test = verb_extractor.transform(X_test, verbs_voc, 'VV')\n",
    "mod_aux_voc = mod_aux_extractor.fit(X_train, 'VM')\n",
    "mod_aux_vec_train = mod_aux_extractor.transform(X_train, mod_aux_voc, 'VM')\n",
    "mod_aux_vec_test = mod_aux_extractor.transform(X_test, mod_aux_voc, 'VM')\n",
    "sent_stat_train = sent_stat_extractor.transform(X_train)\n",
    "sent_stat_test = sent_stat_extractor.transform(X_test)\n",
    "pun_train = pun_extractor.transform(X_train)\n",
    "pun_test = pun_extractor.transform(X_test)\n",
    "all_train = []\n",
    "all_test = []\n",
    "    \n",
    "for i in range(len(X_train)):\n",
    "#     all_train.append(unigrams_train[i] + bigrams_train[i] + trigrams_train[i] + adverbs_vec_train[i] + verbs_vec_train[i] + mod_aux_vec_train[i] + sent_stat_train[i] + pun_train[i])\n",
    "    all_train.append(unigrams_train[i] + adverbs_vec_train[i] + verbs_vec_train[i] + mod_aux_vec_train[i] + sent_stat_train[i] + pun_train[i])\n",
    "\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "#     all_test.append(unigrams_test[i] + bigrams_test[i] + trigrams_test[i] + adverbs_vec_test[i] + verbs_vec_test[i] + mod_aux_vec_test[i] + sent_stat_test[i] + pun_test[i])\n",
    "    all_test.append(unigrams_test[i] + adverbs_vec_test[i] + verbs_vec_test[i] + mod_aux_vec_test[i] + sent_stat_test[i] + pun_test[i])\n",
    "    \n",
    "clf = LogisticRegression().fit(all_train, y_train)\n",
    "predict = clf.predict(all_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Prediction 1\n",
    "\n",
    "In this step the best feature from the training and evaluation step is chosen and predictions are done on the 80/20 split.  \n",
    "Furthermore, the trained model is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7bfcc37f3eee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvectorizer_uni\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0munigrams_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer_uni\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0munigrams_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer_uni\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# unigrams\n",
    "# prediction with 80/20 split\n",
    "\n",
    "vectorizer_uni = CountVectorizer(analyzer='word', lowercase=False, ngram_range=(1, 1))\n",
    "unigrams_train = vectorizer_uni.fit_transform(X_train)\n",
    "unigrams_test = vectorizer_uni.transform(X_test)\n",
    "\n",
    "clf_uni = LogisticRegression(max_iter=1000).fit(unigrams_train, y_train)\n",
    "uni_predict = clf_uni.predict(unigrams_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, uni_predict))\n",
    "\n",
    "with open('classifier_one.pkl', 'wb') as file:\n",
    "    pickle.dump(clf_uni, file)\n",
    "    \n",
    "with open('vectorizer_uni.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer_uni, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparetation and Feature Engineering 2\n",
    "\n",
    "In this step the data is loaded again and a ferature vector for the second classification is composed. It consists of the following features:\n",
    "- prediction of the first classifier, every not interesting feature is dropped\n",
    "- absolout sentence location in decision reasons\n",
    "- absolout sentence location in paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus data, flatten corpus, delete all not relevant data, extract position information for each sentence and add decision of first classifier\n",
    "\n",
    "path = 'thesis_corpus/'\n",
    "\n",
    "with open('classifier_one.pkl', 'rb') as file:\n",
    "    clf_one = pickle.load(file)\n",
    "with open('vectorizer_uni.pkl', 'rb') as file:\n",
    "    vectorizer_uni = pickle.load(file)\n",
    "    \n",
    "corpus = []\n",
    "corpus_labels = []\n",
    "feature_vector = []\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    with open(path+file, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        reasons = data['decision_text']['decision_reasons']\n",
    "    \n",
    "    doc_sent = 0\n",
    "    \n",
    "    for reason in reasons:\n",
    "        for i in range(len(reason)):\n",
    "            doc_sent += 1\n",
    "            sent_len = 0\n",
    "            if reason[i][1] != 'other':\n",
    "                corpus.append(reason[i][0])\n",
    "                corpus_labels.append(reason[i][1])\n",
    "                if len(reason[i][0].split(' ')) > 12:\n",
    "                    sent_len = 1\n",
    "                feature_vector.append([i, doc_sent, sent_len])\n",
    "        \n",
    "           \n",
    "unigrams = vectorizer_uni.transform(corpus)\n",
    "predictions_one = clf_one.predict(unigrams)\n",
    "\n",
    "for i in range(len(feature_vector)):\n",
    "    if predictions_one[i] == 'no_interest':\n",
    "        feature_vector[i].append(0)\n",
    "    else:\n",
    "        feature_vector[i].append(1)\n",
    "\n",
    "                        \n",
    "X = np.array(corpus)\n",
    "y = np.array(corpus_labels)\n",
    "indeces = [i for i in range(len(X))]\n",
    "                \n",
    "# train/test split with 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(indeces), y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.28      0.96      0.43      1057\n",
      " subsumption       0.65      0.03      0.06      2687\n",
      "\n",
      "    accuracy                           0.29      3744\n",
      "   macro avg       0.47      0.49      0.25      3744\n",
      "weighted avg       0.55      0.29      0.17      3744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_train = [feature_vector[index] for index in X_train]\n",
    "features_test = [feature_vector[index] for index in X_test]\n",
    "\n",
    "clf_two = LinearSVC().fit(features_train, y_train)\n",
    "predict = clf_two.predict(features_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.00      0.00      0.00       841\n",
      " subsumption       0.72      1.00      0.84      2154\n",
      "\n",
      "    accuracy                           0.72      2995\n",
      "   macro avg       0.36      0.50      0.42      2995\n",
      "weighted avg       0.52      0.72      0.60      2995\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.00      0.00      0.00       851\n",
      " subsumption       0.72      1.00      0.83      2144\n",
      "\n",
      "    accuracy                           0.72      2995\n",
      "   macro avg       0.36      0.50      0.42      2995\n",
      "weighted avg       0.51      0.72      0.60      2995\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.00      0.00      0.00       835\n",
      " subsumption       0.72      1.00      0.84      2160\n",
      "\n",
      "    accuracy                           0.72      2995\n",
      "   macro avg       0.36      0.50      0.42      2995\n",
      "weighted avg       0.52      0.72      0.60      2995\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.00      0.00      0.00       846\n",
      " subsumption       0.72      1.00      0.84      2148\n",
      "\n",
      "    accuracy                           0.72      2994\n",
      "   macro avg       0.36      0.50      0.42      2994\n",
      "weighted avg       0.51      0.72      0.60      2994\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.00      0.00      0.00       843\n",
      " subsumption       0.72      1.00      0.84      2151\n",
      "\n",
      "    accuracy                           0.72      2994\n",
      "   macro avg       0.36      0.50      0.42      2994\n",
      "weighted avg       0.52      0.72      0.60      2994\n",
      "\n",
      "Accuracy: 0.7184264355087471\n",
      "Precision: 0.35921321775437354\n",
      "Recall: 0.5\n",
      "F1-Score: 0.4180716415272661\n"
     ]
    }
   ],
   "source": [
    "kf = KFold()\n",
    "\n",
    "acc = 0\n",
    "pre = 0\n",
    "rec = 0\n",
    "f_1 = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_training, X_testing = X_train[train_index], X_train[test_index]\n",
    "    y_training, y_testing = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    features_train = [feature_vector[index] for index in X_training]\n",
    "    features_test = [feature_vector[index] for index in X_testing]\n",
    "\n",
    "    clf_two = LinearSVC().fit(features_train, y_training)\n",
    "    predict = clf_two.predict(features_test)\n",
    "\n",
    "    acc += metrics.accuracy_score(y_testing, predict)   \n",
    "    metric = metrics.precision_recall_fscore_support(y_testing, predict, average='macro')\n",
    "    pre += metric[0]\n",
    "    rec += metric[1]\n",
    "    f_1 += metric[2]\n",
    "    print(metrics.classification_report(y_testing, predict))\n",
    "    \n",
    "print('Accuracy: ' + str(acc/5))\n",
    "print('Precision: ' + str(pre/5))\n",
    "print('Recall: ' + str(rec/5))\n",
    "print('F1-Score: ' + str(f_1/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with own approach\n",
    "\n",
    "The baseline model and features did not bring good results. To be able to work with the trained model a new approach is implemented.  \n",
    "Unigrams seem to be a good indecator for classification. Therefore this feature is used in a multi-class calssification approach. Both Logistic Regression and SVM are tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'thesis_corpus/'\n",
    "\n",
    "# flatten corpus for generating basic features and extract flattende labels\n",
    "corpus = []\n",
    "corpus_labels = []\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    with open(path+file, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        reasons = data['decision_text']['decision_reasons']\n",
    "    \n",
    "    for reason in reasons:\n",
    "        for sentence in reason:\n",
    "            corpus.append(sentence[0])\n",
    "            corpus_labels.append(sentence[1])\n",
    "        \n",
    "X = np.array(corpus)\n",
    "y = np.array(corpus_labels)\n",
    "                \n",
    "# train/test split with 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.77      0.70      0.73       861\n",
      "       other       0.72      0.72      0.72      1023\n",
      " subsumption       0.80      0.83      0.82      2148\n",
      "\n",
      "    accuracy                           0.78      4032\n",
      "   macro avg       0.77      0.75      0.76      4032\n",
      "weighted avg       0.78      0.78      0.78      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.78      0.71      0.74       815\n",
      "       other       0.72      0.72      0.72      1005\n",
      " subsumption       0.81      0.84      0.82      2212\n",
      "\n",
      "    accuracy                           0.78      4032\n",
      "   macro avg       0.77      0.76      0.76      4032\n",
      "weighted avg       0.78      0.78      0.78      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.77      0.70      0.73       859\n",
      "       other       0.73      0.70      0.71      1031\n",
      " subsumption       0.79      0.83      0.81      2142\n",
      "\n",
      "    accuracy                           0.77      4032\n",
      "   macro avg       0.76      0.75      0.75      4032\n",
      "weighted avg       0.77      0.77      0.77      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.80      0.69      0.74       851\n",
      "       other       0.73      0.73      0.73      1037\n",
      " subsumption       0.80      0.84      0.82      2143\n",
      "\n",
      "    accuracy                           0.78      4031\n",
      "   macro avg       0.78      0.76      0.77      4031\n",
      "weighted avg       0.78      0.78      0.78      4031\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.79      0.72      0.75       866\n",
      "       other       0.73      0.72      0.72      1045\n",
      " subsumption       0.80      0.84      0.82      2120\n",
      "\n",
      "    accuracy                           0.78      4031\n",
      "   macro avg       0.77      0.76      0.77      4031\n",
      "weighted avg       0.78      0.78      0.78      4031\n",
      "\n",
      "Precision: 0.7700864367782314\n",
      "Recall: 0.7534122257739915\n",
      "F1-Score: 0.7608906886947481\n",
      "Accuracy: 0.7786986297661379\n"
     ]
    }
   ],
   "source": [
    "# Unigram and Logistic Regression\n",
    "\n",
    "kf = KFold()\n",
    "\n",
    "acc = 0\n",
    "pre = 0\n",
    "rec = 0\n",
    "f_1 = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_training, X_testing = X_train[train_index], X_train[test_index]\n",
    "    y_training, y_testing = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    vectorizer_uni = CountVectorizer(analyzer='word', lowercase=False, ngram_range=(1, 1))\n",
    "    unigrams_train = vectorizer_uni.fit_transform(X_training)\n",
    "    unigrams_test = vectorizer_uni.transform(X_testing)\n",
    "\n",
    "    clf_uni = LogisticRegression( max_iter=1000).fit(unigrams_train, y_training)\n",
    "    uni_predict = clf_uni.predict(unigrams_test)\n",
    "\n",
    "    acc += metrics.accuracy_score(y_testing, uni_predict)   \n",
    "    metric = metrics.precision_recall_fscore_support(y_testing, uni_predict, average='macro')\n",
    "    pre += metric[0]\n",
    "    rec += metric[1]\n",
    "    f_1 += metric[2]\n",
    "    print(metrics.classification_report(y_testing, uni_predict))\n",
    "    \n",
    "print('Precision: ' + str(pre/5))\n",
    "print('Recall: ' + str(rec/5))\n",
    "print('F1-Score: ' + str(f_1/5))\n",
    "print('Accuracy: ' + str(acc/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.75      0.67      0.71       861\n",
      "       other       0.66      0.73      0.69      1023\n",
      " subsumption       0.80      0.79      0.79      2148\n",
      "\n",
      "    accuracy                           0.75      4032\n",
      "   macro avg       0.74      0.73      0.73      4032\n",
      "weighted avg       0.75      0.75      0.75      4032\n",
      "\n",
      "32066\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.74      0.69      0.72       815\n",
      "       other       0.67      0.73      0.70      1005\n",
      " subsumption       0.80      0.79      0.80      2212\n",
      "\n",
      "    accuracy                           0.75      4032\n",
      "   macro avg       0.74      0.74      0.74      4032\n",
      "weighted avg       0.76      0.75      0.76      4032\n",
      "\n",
      "32257\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.75      0.68      0.71       859\n",
      "       other       0.66      0.71      0.68      1031\n",
      " subsumption       0.79      0.78      0.78      2142\n",
      "\n",
      "    accuracy                           0.74      4032\n",
      "   macro avg       0.73      0.73      0.73      4032\n",
      "weighted avg       0.74      0.74      0.74      4032\n",
      "\n",
      "32231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.79      0.68      0.73       851\n",
      "       other       0.68      0.75      0.71      1037\n",
      " subsumption       0.80      0.80      0.80      2143\n",
      "\n",
      "    accuracy                           0.76      4031\n",
      "   macro avg       0.75      0.74      0.75      4031\n",
      "weighted avg       0.76      0.76      0.76      4031\n",
      "\n",
      "32491\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.76      0.70      0.73       866\n",
      "       other       0.67      0.73      0.70      1045\n",
      " subsumption       0.80      0.79      0.80      2120\n",
      "\n",
      "    accuracy                           0.76      4031\n",
      "   macro avg       0.74      0.74      0.74      4031\n",
      "weighted avg       0.76      0.76      0.76      4031\n",
      "\n",
      "Precision: 0.7400457259466645\n",
      "Recall: 0.7350456398949401\n",
      "F1-Score: 0.7364014261555079\n",
      "Accuracy: 0.7527538191121979\n"
     ]
    }
   ],
   "source": [
    "# Unigram and SVM\n",
    "\n",
    "kf = KFold()\n",
    "\n",
    "acc = 0\n",
    "pre = 0\n",
    "rec = 0\n",
    "f_1 = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_training, X_testing = X_train[train_index], X_train[test_index]\n",
    "    y_training, y_testing = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    vectorizer_uni = CountVectorizer(analyzer='word', lowercase=False, ngram_range=(1, 1))\n",
    "    unigrams_train = vectorizer_uni.fit_transform(X_training)\n",
    "    print(len(vectorizer_uni.get_feature_names()))\n",
    "    unigrams_test = vectorizer_uni.transform(X_testing)\n",
    "\n",
    "    clf_uni = LinearSVC().fit(unigrams_train, y_training)\n",
    "    uni_predict = clf_uni.predict(unigrams_test)\n",
    "\n",
    "    acc += metrics.accuracy_score(y_testing, uni_predict)   \n",
    "    metric = metrics.precision_recall_fscore_support(y_testing, uni_predict, average='macro')\n",
    "    pre += metric[0]\n",
    "    rec += metric[1]\n",
    "    f_1 += metric[2]\n",
    "    print(metrics.classification_report(y_testing, uni_predict))\n",
    "    \n",
    "print('Precision: ' + str(pre/5))\n",
    "print('Recall: ' + str(rec/5))\n",
    "print('F1-Score: ' + str(f_1/5))\n",
    "print('Accuracy: ' + str(acc/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.80      0.67      0.73       861\n",
      "       other       0.80      0.64      0.71      1023\n",
      " subsumption       0.77      0.89      0.83      2148\n",
      "\n",
      "    accuracy                           0.78      4032\n",
      "   macro avg       0.79      0.74      0.76      4032\n",
      "weighted avg       0.78      0.78      0.78      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.80      0.68      0.74       815\n",
      "       other       0.82      0.64      0.72      1005\n",
      " subsumption       0.78      0.90      0.83      2212\n",
      "\n",
      "    accuracy                           0.79      4032\n",
      "   macro avg       0.80      0.74      0.76      4032\n",
      "weighted avg       0.79      0.79      0.78      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.79      0.67      0.72       859\n",
      "       other       0.82      0.63      0.71      1031\n",
      " subsumption       0.76      0.89      0.82      2142\n",
      "\n",
      "    accuracy                           0.78      4032\n",
      "   macro avg       0.79      0.73      0.75      4032\n",
      "weighted avg       0.78      0.78      0.77      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.84      0.68      0.75       851\n",
      "       other       0.83      0.66      0.74      1037\n",
      " subsumption       0.77      0.90      0.83      2143\n",
      "\n",
      "    accuracy                           0.79      4031\n",
      "   macro avg       0.81      0.75      0.77      4031\n",
      "weighted avg       0.80      0.79      0.79      4031\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.83      0.67      0.74       866\n",
      "       other       0.82      0.64      0.72      1045\n",
      " subsumption       0.77      0.91      0.83      2120\n",
      "\n",
      "    accuracy                           0.79      4031\n",
      "   macro avg       0.80      0.74      0.76      4031\n",
      "weighted avg       0.79      0.79      0.78      4031\n",
      "\n",
      "Precision: 0.7991112481284477\n",
      "Recall: 0.7383270560098012\n",
      "F1-Score: 0.7613612626245083\n",
      "Accuracy: 0.7857927451142535\n"
     ]
    }
   ],
   "source": [
    "# tf-idf and Logistic Regression\n",
    "\n",
    "kf = KFold()\n",
    "\n",
    "acc = 0\n",
    "pre = 0\n",
    "rec = 0\n",
    "f_1 = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_training, X_testing = X_train[train_index], X_train[test_index]\n",
    "    y_training, y_testing = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    vectorizer_tfidf = TfidfVectorizer(lowercase=False)\n",
    "    tfidf_train = vectorizer_tfidf.fit_transform(X_training)\n",
    "    tfidf_test = vectorizer_tfidf.transform(X_testing)\n",
    "\n",
    "    clf_tfidf = LogisticRegression(max_iter=1000).fit(tfidf_train, y_training)\n",
    "    tfidf_predict = clf_tfidf.predict(tfidf_test)\n",
    "\n",
    "    acc += metrics.accuracy_score(y_testing, tfidf_predict)   \n",
    "    metric = metrics.precision_recall_fscore_support(y_testing, tfidf_predict, average='macro')\n",
    "    pre += metric[0]\n",
    "    rec += metric[1]\n",
    "    f_1 += metric[2]\n",
    "    print(metrics.classification_report(y_testing, tfidf_predict))\n",
    "    \n",
    "print('Precision: ' + str(pre/5))\n",
    "print('Recall: ' + str(rec/5))\n",
    "print('F1-Score: ' + str(f_1/5))\n",
    "print('Accuracy: ' + str(acc/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.78      0.71      0.74       861\n",
      "       other       0.76      0.69      0.72      1023\n",
      " subsumption       0.80      0.86      0.83      2148\n",
      "\n",
      "    accuracy                           0.78      4032\n",
      "   macro avg       0.78      0.75      0.76      4032\n",
      "weighted avg       0.78      0.78      0.78      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.76      0.72      0.74       815\n",
      "       other       0.77      0.69      0.73      1005\n",
      " subsumption       0.80      0.86      0.83      2212\n",
      "\n",
      "    accuracy                           0.79      4032\n",
      "   macro avg       0.78      0.76      0.77      4032\n",
      "weighted avg       0.79      0.79      0.79      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.78      0.73      0.75       859\n",
      "       other       0.76      0.68      0.72      1031\n",
      " subsumption       0.79      0.85      0.82      2142\n",
      "\n",
      "    accuracy                           0.78      4032\n",
      "   macro avg       0.77      0.75      0.76      4032\n",
      "weighted avg       0.78      0.78      0.78      4032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.80      0.72      0.75       851\n",
      "       other       0.78      0.70      0.74      1037\n",
      " subsumption       0.79      0.86      0.82      2143\n",
      "\n",
      "    accuracy                           0.79      4031\n",
      "   macro avg       0.79      0.76      0.77      4031\n",
      "weighted avg       0.79      0.79      0.79      4031\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.79      0.74      0.76       866\n",
      "       other       0.77      0.69      0.73      1045\n",
      " subsumption       0.80      0.86      0.83      2120\n",
      "\n",
      "    accuracy                           0.79      4031\n",
      "   macro avg       0.79      0.76      0.77      4031\n",
      "weighted avg       0.79      0.79      0.79      4031\n",
      "\n",
      "Precision: 0.7808805647305566\n",
      "Recall: 0.7571201619319521\n",
      "F1-Score: 0.7676269096235376\n",
      "Accuracy: 0.7862886784168724\n"
     ]
    }
   ],
   "source": [
    "# tfidf and SVM\n",
    "\n",
    "kf = KFold()\n",
    "\n",
    "acc = 0\n",
    "pre = 0\n",
    "rec = 0\n",
    "f_1 = 0\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_training, X_testing = X_train[train_index], X_train[test_index]\n",
    "    y_training, y_testing = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    vectorizer_tfidf = TfidfVectorizer(lowercase=False)\n",
    "    tfidf_train = vectorizer_tfidf.fit_transform(X_training)\n",
    "    tfidf_test = vectorizer_tfidf.transform(X_testing)\n",
    "\n",
    "    clf_tfidf = LinearSVC().fit(tfidf_train, y_training)\n",
    "    tfidf_predict = clf_tfidf.predict(tfidf_test)\n",
    "\n",
    "    acc += metrics.accuracy_score(y_testing, tfidf_predict)   \n",
    "    metric = metrics.precision_recall_fscore_support(y_testing, tfidf_predict, average='macro')\n",
    "    pre += metric[0]\n",
    "    rec += metric[1]\n",
    "    f_1 += metric[2]\n",
    "    print(metrics.classification_report(y_testing, tfidf_predict))\n",
    "    \n",
    "print('Precision: ' + str(pre/5))\n",
    "print('Recall: ' + str(rec/5))\n",
    "print('F1-Score: ' + str(f_1/5))\n",
    "print('Accuracy: ' + str(acc/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.77      0.69      0.73      1060\n",
      "       other       0.78      0.71      0.74      1281\n",
      " subsumption       0.80      0.86      0.83      2699\n",
      "\n",
      "    accuracy                           0.79      5040\n",
      "   macro avg       0.78      0.75      0.77      5040\n",
      "weighted avg       0.79      0.79      0.78      5040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf-idf SVM\n",
    "# prediction with 80/20 split\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(lowercase=False)\n",
    "tfidf_train = vectorizer_tfidf.fit_transform(X_train)\n",
    "tfidf_test = vectorizer_tfidf.transform(X_test)\n",
    "\n",
    "clf_tfidf = LinearSVC().fit(tfidf_train, y_train)\n",
    "tfidf_predict = clf_tfidf.predict(tfidf_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, tfidf_predict))\n",
    "\n",
    "with open('classifier_own.pkl', 'wb') as file:\n",
    "    pickle.dump(clf_tfidf, file)\n",
    "    \n",
    "with open('vectorizer_tfidf.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer_tfidf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  definition       0.80      0.65      0.72      1060\n",
      "       other       0.81      0.65      0.73      1281\n",
      " subsumption       0.77      0.90      0.83      2699\n",
      "\n",
      "    accuracy                           0.78      5040\n",
      "   macro avg       0.79      0.74      0.76      5040\n",
      "weighted avg       0.79      0.78      0.78      5040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf-idf Logistic Regression\n",
    "# prediction with 80/20 split\n",
    "\n",
    "tfidf_train = vectorizer_tfidf.transform(X_train)\n",
    "tfidf_test = vectorizer_tfidf.transform(X_test)\n",
    "\n",
    "clf_tfidf = LogisticRegression(max_iter=1000).fit(tfidf_train, y_train)\n",
    "tfidf_predict = clf_tfidf.predict(tfidf_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, tfidf_predict))\n",
    "\n",
    "with open('classifier_own_2.pkl', 'wb') as file:\n",
    "    pickle.dump(clf_tfidf, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
